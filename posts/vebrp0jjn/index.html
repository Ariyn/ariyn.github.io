<!doctype html>
<html lang="en">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="Liste - https://ariyn.github.io/">
  <title>mediapipe를 사용해 모션캡쳐하기 | this.isMin.uk</title>
  <meta name="description" content="Minimalistic Hugo blogging theme">
<meta property="og:title" content="mediapipe를 사용해 모션캡쳐하기" />
<meta property="og:description" content="mediapipe는 구글에서 만든 모션 캡쳐 라이브러리다. 기존의 모션 캡쳐 라이브러리가 2D에서 2D를 인식하는 한계가 있었다면, 이 라이브러리는 2D이미지를 가지고 3D를 인식하는 것이 목표이다. python, android, ios, C&#43;&#43; 버전이 있으며, 온 디바이스에서 바로 인식하는것을 목표로 개발중이다. Pose, Hand, Face, Object 트래킹등 모션 캡쳐에 필요한 대부분의 기능이 포함되어 있고, 여러가지를 동시에 사용할 수도 있다. 현재는 ML이 적용된 새로운 버전이 나오고 있다. 일부 기능에만 ML이 적용된 버전이 있는데, 모든 기능에 대해서 나온다면 훨씬 좋은 성능을 낼 것으로 기대된다." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ariyn.github.io/posts/vebrp0jjn/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-02T20:35:02+07:00" />
<meta property="article:modified_time" content="2023-04-02T21:24:15+07:00" />


  
  <meta itemprop="name" content="mediapipe를 사용해 모션캡쳐하기">
<meta itemprop="description" content="mediapipe는 구글에서 만든 모션 캡쳐 라이브러리다. 기존의 모션 캡쳐 라이브러리가 2D에서 2D를 인식하는 한계가 있었다면, 이 라이브러리는 2D이미지를 가지고 3D를 인식하는 것이 목표이다. python, android, ios, C&#43;&#43; 버전이 있으며, 온 디바이스에서 바로 인식하는것을 목표로 개발중이다. Pose, Hand, Face, Object 트래킹등 모션 캡쳐에 필요한 대부분의 기능이 포함되어 있고, 여러가지를 동시에 사용할 수도 있다. 현재는 ML이 적용된 새로운 버전이 나오고 있다. 일부 기능에만 ML이 적용된 버전이 있는데, 모든 기능에 대해서 나온다면 훨씬 좋은 성능을 낼 것으로 기대된다."><meta itemprop="datePublished" content="2023-04-02T20:35:02+07:00" />
<meta itemprop="dateModified" content="2023-04-02T21:24:15+07:00" />
<meta itemprop="wordCount" content="393">
<meta itemprop="keywords" content="개발,mediapipe," />
  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="mediapipe를 사용해 모션캡쳐하기"/>
<meta name="twitter:description" content="mediapipe는 구글에서 만든 모션 캡쳐 라이브러리다. 기존의 모션 캡쳐 라이브러리가 2D에서 2D를 인식하는 한계가 있었다면, 이 라이브러리는 2D이미지를 가지고 3D를 인식하는 것이 목표이다. python, android, ios, C&#43;&#43; 버전이 있으며, 온 디바이스에서 바로 인식하는것을 목표로 개발중이다. Pose, Hand, Face, Object 트래킹등 모션 캡쳐에 필요한 대부분의 기능이 포함되어 있고, 여러가지를 동시에 사용할 수도 있다. 현재는 ML이 적용된 새로운 버전이 나오고 있다. 일부 기능에만 ML이 적용된 버전이 있는데, 모든 기능에 대해서 나온다면 훨씬 좋은 성능을 낼 것으로 기대된다."/>

  <link rel="canonical" href="https://ariyn.github.io/posts/vebrp0jjn/">
  <meta name="monetization" content="$twitter.xrptipbot.com/ronaldsvilcins">
  <link rel="dns-prefetch" href="https://www.google-analytics.com">
  <link href="https://www.google-analytics.com" rel="preconnect" crossorigin>
  <link rel="alternate" type="application/atom+xml" title="this.isMin.uk" href="https://ariyn.github.io/atom.xml" />
  <link rel="alternate" type="application/json" title="this.isMin.uk" href="https://ariyn.github.io/feed.json" />
  <link rel="shortcut icon" type="image/png" href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNk+A8AAQUBAScY42YAAAAASUVORK5CYII=">
  
  <style>*,:after,:before{box-sizing:border-box;padding:0}body{font:1rem/1.5 '-apple-system',BlinkMacSystemFont,avenir next,avenir,helvetica,helvetica neue,ubuntu,roboto,noto,segoe ui,arial,sans-serif;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;padding:2rem;color:#000}.skip-link{position:absolute;top:-40px;left:0;background:#eee;z-index:100}.skip-link:focus{top:0}h1,h2,h3,h4,h5,strong,b{font-size:inherit;font-weight:600}header{line-height:2;padding-bottom:1.5rem}.link{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.time{font-variant-numeric:tabular-nums;white-space:nowrap}blockquote{border-left:5px solid #eee;padding-left:1rem;margin:0}a,a:visited{color:inherit}a:hover,a.heading-link{text-decoration:none}pre{padding:.5rem;overflow:auto;overflow-x:scroll;overflow-wrap:normal}code,pre{font-family:San Francisco Mono,Monaco,consolas,lucida console,dejavu sans mono,bitstream vera sans mono,monospace;font-size:normal;font-size:small;background:#eee}code{padding:.1rem;border:none}ul{list-style-type:square}ul,ol{padding-left:1.2rem}.list{line-height:2;list-style-type:none;padding-left:0}.list li{padding-bottom:.1rem}.meta{color:#777}.content{max-width:70ch;margin:0 auto}header{line-height:2;display:flex;justify-content:space-between;padding-bottom:1rem}header a{text-decoration:none}header ul{list-style-type:none;padding:0}header li,header a{display:inline}h2.post{padding-top:.5rem}header ul a:first-child{padding-left:1rem}.nav{height:1px;background:#000;content:'';max-width:10%}.list li{display:flex;align-items:baseline}.list li time{flex:initial}.hr-list{margin-top:0;margin-bottom:0;margin-right:.5rem;margin-left:.5rem;height:1px;border:0;border-bottom:1px dotted #ccc;flex:1 0 1rem}.m,hr{border:0;margin:3rem 0}img{max-width:100%;height:auto}</style>
  <link rel="stylesheet" href="/css/codeblock.css" />

  


<script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "articleSection": "posts",
    "name": "mediapipe를 사용해 모션캡쳐하기",
    "headline": "mediapipe를 사용해 모션캡쳐하기",
    "alternativeHeadline": "",
    "description": "mediapipe는 구글에서 만든 모션 캡쳐 라이브러리다. 기존의 모션 캡쳐 라이브러리가 2D에서 2D를 인식하는 한계가 있었다면, 이 라이브러리는 2D이미지를 가지고 3D를 인식하는 것이 목표이다. python, android, ios, C\u002b\u002b 버전이 있으며, 온 디바이스에서 바로 인식하는것을 목표로 개발중이다. Pose, Hand, Face, Object 트래킹등 모션 캡쳐에 필요한 대부분의 기능이 포함되어 있고, 여러가지를 동시에 사용할 수도 있다. 현재는 ML이 적용된 새로운 버전이 나오고 있다. 일부 기능에만 ML이 적용된 버전이 있는데, 모든 기능에 대해서 나온다면 훨씬 좋은 성능을 낼 것으로 기대된다.",
    "inLanguage": "en-us",
    "isFamilyFriendly": "true",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https:\/\/ariyn.github.io\/posts\/vebrp0jjn\/"
    },
    "author" : {
        "@type": "Person",
        "name": ""
    },
    "creator" : {
        "@type": "Person",
        "name": ""
    },
    "accountablePerson" : {
        "@type": "Person",
        "name": ""
    },
    "copyrightHolder" : "this.isMin.uk",
    "copyrightYear" : "2023",
    "dateCreated": "2023-04-02T20:35:02.00Z",
    "datePublished": "2023-04-02T20:35:02.00Z",
    "dateModified": "2023-04-02T21:24:15.00Z",
    "publisher":{
        "@type":"Organization",
        "name": "this.isMin.uk",
        "url": "https://ariyn.github.io/",
        "logo": {
            "@type": "ImageObject",
            "url": "https:\/\/ariyn.github.io\/",
            "width":"32",
            "height":"32"
        }
    },
    "image": "https://ariyn.github.io/",
    "url" : "https:\/\/ariyn.github.io\/posts\/vebrp0jjn\/",
    "wordCount" : "393",
    "genre" : [ "개발" , "mediapipe" ],
    "keywords" : [ "개발" , "mediapipe" ]
}
</script>


</head>
<body>
  <a class="skip-link" href="#main">Skip to main</a>
<main id="main">
  <div class="content">
    <header>
<p style="padding: 0;margin: 0;"><a href="/"><b>this.isMin.uk</b></a></p>
<ul style="padding: 0;margin: 0;">
  <li><a href="/about">About</a></li> 
  <li><a href="/random">Random</a></li>
</ul>
</header>
<hr class="hr-list" style="padding: 0;margin: 0;">

      
        
      <h2 class="post">mediapipe를 사용해 모션캡쳐하기</h2>
      <ul>
<li><a href="https://developers.google.com/mediapipe">mediapipe</a>는 구글에서 만든 모션 캡쳐 라이브러리다.
<ul>
<li>기존의 모션 캡쳐 라이브러리가 2D에서 2D를 인식하는 한계가 있었다면, 이 라이브러리는 2D이미지를 가지고 3D를 인식하는 것이 목표이다.
<ul>
<li>python, android, ios, C++ 버전이 있으며, 온 디바이스에서 바로 인식하는것을 목표로 개발중이다.</li>
<li>Pose, Hand, Face, Object 트래킹등 모션 캡쳐에 필요한 대부분의 기능이 포함되어 있고, 여러가지를 동시에 사용할 수도 있다.</li>
</ul>
</li>
<li>현재는 ML이 적용된 새로운 버전이 나오고 있다.
<ul>
<li>일부 기능에만 ML이 적용된 버전이 있는데, 모든 기능에 대해서 나온다면 훨씬 좋은 성능을 낼 것으로 기대된다.</li>
</ul>
</li>
</ul>
</li>
<li>필요 라이브러리
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plain" data-lang="plain"><span style="display:flex;"><span>numpy
</span></span><span style="display:flex;"><span>opencv-python
</span></span><span style="display:flex;"><span>mediapipe
</span></span></code></pre></div></li>
<li>사용방법
<ul>
<li>Legacy와 새로운 버전에 따라 사용방법이 조금 다르다</li>
<li>legacy 버전 사용 방법 (pose)
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> cv2
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> mediapipe <span style="color:#66d9ef">as</span> mp
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> mediapipe.python.solutions <span style="color:#f92672">import</span> drawing_utils <span style="color:#66d9ef">as</span> mp_drawing
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> mediapipe.python.solutions <span style="color:#f92672">import</span> pose <span style="color:#66d9ef">as</span> mp_pose
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>image <span style="color:#f92672">=</span> mp<span style="color:#f92672">.</span>Image<span style="color:#f92672">.</span>create_from_file(<span style="color:#e6db74">&#34;image.jpg&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> mp_pose<span style="color:#f92672">.</span>Pose() <span style="color:#66d9ef">as</span> pose_tracker:
</span></span><span style="display:flex;"><span>  result <span style="color:#f92672">=</span> pose_tracker<span style="color:#f92672">.</span>process(image<span style="color:#f92672">=</span>image)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>overlay_image <span style="color:#f92672">=</span> image<span style="color:#f92672">.</span>copy()
</span></span><span style="display:flex;"><span>pl <span style="color:#f92672">=</span> result<span style="color:#f92672">.</span>pose_landmarks
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>mp_drawing<span style="color:#f92672">.</span>draw_landmarks(
</span></span><span style="display:flex;"><span>    image<span style="color:#f92672">=</span>overlay_image,
</span></span><span style="display:flex;"><span>    landmark_list<span style="color:#f92672">=</span>pl,
</span></span><span style="display:flex;"><span>    connections<span style="color:#f92672">=</span>mp_pose<span style="color:#f92672">.</span>POSE_CONNECTIONS)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cv2<span style="color:#f92672">.</span>imwrite(path, cv2<span style="color:#f92672">.</span>cvtColor(overlay_image, cv2<span style="color:#f92672">.</span>COLOR_RGB2BGR))
</span></span></code></pre></div></li>
<li><a href="https://developers.google.com/mediapipe/solutions/examples">신버전</a> 사용 방법
<ul>
<li>신 버전은 model을 다운받아야 하며, 아직까지 Hand, Object Detection등 제한된 기능만 가능하다. <a href="https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/hand_landmarker/python/hand_landmarker.ipynb#scrollTo=Iy4r2_ePylIa">출처</a></li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># STEP 1: Import the necessary modules.</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> mediapipe <span style="color:#66d9ef">as</span> mp
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> mediapipe.tasks <span style="color:#f92672">import</span> python
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> mediapipe.tasks.python <span style="color:#f92672">import</span> vision
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># STEP 2: Create an ImageClassifier object.</span>
</span></span><span style="display:flex;"><span>base_options <span style="color:#f92672">=</span> python<span style="color:#f92672">.</span>BaseOptions(model_asset_path<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;hand_landmarker.task&#39;</span>)
</span></span><span style="display:flex;"><span>options <span style="color:#f92672">=</span> vision<span style="color:#f92672">.</span>HandLandmarkerOptions(base_options<span style="color:#f92672">=</span>base_options,
</span></span><span style="display:flex;"><span>                                       num_hands<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>detector <span style="color:#f92672">=</span> vision<span style="color:#f92672">.</span>HandLandmarker<span style="color:#f92672">.</span>create_from_options(options)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># STEP 3: Load the input image.</span>
</span></span><span style="display:flex;"><span>image <span style="color:#f92672">=</span> mp<span style="color:#f92672">.</span>Image<span style="color:#f92672">.</span>create_from_file(<span style="color:#e6db74">&#34;image.jpg&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># STEP 4: Detect hand landmarks from the input image.</span>
</span></span><span style="display:flex;"><span>detection_result <span style="color:#f92672">=</span> detector<span style="color:#f92672">.</span>detect(image)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># STEP 5: Process the classification result. In this case, visualize it.</span>
</span></span><span style="display:flex;"><span>annotated_image <span style="color:#f92672">=</span> draw_landmarks_on_image(image<span style="color:#f92672">.</span>numpy_view(), detection_result)
</span></span><span style="display:flex;"><span>cv2_imshow(cv2<span style="color:#f92672">.</span>cvtColor(annotated_image, cv2<span style="color:#f92672">.</span>COLOR_RGB2BGR))
</span></span></code></pre></div></li>
<li>동영상의 경우, 프레임 단위로 이미지를 분리한 뒤 모든 이미지에 대해 동일하게 사용하면 된다.</li>
<li>결과물의 landmark를 사용해서 모션캡쳐 데이터를 가져올 수 있다.
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>result <span style="color:#f92672">=</span> pose_tracker<span style="color:#f92672">.</span>process(image<span style="color:#f92672">=</span>image) <span style="color:#75715e"># Legacy version</span>
</span></span><span style="display:flex;"><span>result <span style="color:#f92672">=</span> detector<span style="color:#f92672">.</span>detect(image) <span style="color:#75715e"># New version</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(result<span style="color:#f92672">.</span>hand_landmarks)
</span></span><span style="display:flex;"><span><span style="color:#f92672">&gt;</span> [[NormalizedLandmark(x<span style="color:#f92672">=</span><span style="color:#ae81ff">0.3925739526748657</span>, y<span style="color:#f92672">=</span><span style="color:#ae81ff">0.7065041661262512</span>, z<span style="color:#f92672">=-</span><span style="color:#ae81ff">3.275009419212438e-07</span>, visibility<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, presence<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>), NormalizedLandmark(x<span style="color:#f92672">=</span><span style="color:#ae81ff">0.3429622948169708</span>, y<span style="color:#f92672">=</span><span style="color:#ae81ff">0.7502768039703369</span>, z<span style="color:#f92672">=-</span><span style="color:#ae81ff">0.01981717348098755</span>, visibility<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, presence<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>), <span style="color:#f92672">...</span>], [NormalizedLandmark(x<span style="color:#f92672">=</span><span style="color:#ae81ff">0.3925739526748657</span>, y<span style="color:#f92672">=</span><span style="color:#ae81ff">0.7065041661262512</span>, z<span style="color:#f92672">=-</span><span style="color:#ae81ff">3.275009419212438e-07</span>, visibility<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, presence<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>), NormalizedLandmark(x<span style="color:#f92672">=</span><span style="color:#ae81ff">0.3429622948169708</span>, y<span style="color:#f92672">=</span><span style="color:#ae81ff">0.7502768039703369</span>, z<span style="color:#f92672">=-</span><span style="color:#ae81ff">0.01981717348098755</span>, visibility<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, presence<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>), <span style="color:#f92672">...</span>]]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(result<span style="color:#f92672">.</span>handness)
</span></span><span style="display:flex;"><span><span style="color:#f92672">&gt;</span> [[Category(index<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, score<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9360456466674805</span>, display_name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Right&#39;</span>, category_name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Right&#39;</span>)], [Category(index<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, score<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9622023105621338</span>, display_name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Left&#39;</span>, category_name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Left&#39;</span>)]]
</span></span></code></pre></div><ul>
<li>Hand Detection의 경우 아래와 같이 landmark가 배열되어 있다.
<ul>
<li>

</li>
</ul>
</li>
<li>PoseDetection의 경우 아래와 같다.
<ul>
<li>

</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>landmark만 가져와서 간단히 처리하고 Unity3D에서 돌리는 모습
 <iframe width="560" height="315" src="https://youtube.com/embed/1ra58BYm72s" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</li>
<li>아직 Z축 검사가 잘 안되는것이 아쉽다.
<ul>
<li>실제로는 팔을 앞뒤로 열심히 흔들었지만, 캡쳐된 모습은 좌우로만 흔드는 모습이다.
 <iframe width="560" height="315" src="https://youtube.com/embed/Q3NlGVuWVuc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</li>
<li>지금은 소프트웨어적으로 처리하는 듯 하다.
<ul>
<li><a href="https://www.mdpi.com/2076-3417/13/4/2700">Human Pose Estimation Using MediaPipe Pose and Optimization Method Based on a Humanoid Model</a> 동아대학교에서 소프트웨어적으로 z 축을 계산한 논문</li>
</ul>
</li>
<li>신 버전에서는 이를 개선하기위해 작업중이라고&hellip; <a href="https://google.github.io/mediapipe/solutions/pose_classification.html#future-work">문서</a></li>
</ul>
</li>
<li>카메라를 두 대 설치하고, 각각 정면, 측면을 촬영해서 하면 어떨까? 싶지만&hellip;
<ul>
<li>테스트삼아 측면으로 찍었을 때 왼팔, 오른팔의 구분이 잘 안되는 문제가 있었다.</li>
<li>그리고 두 개의 카메라의 싱크를 맞추는 것도, 꽤 난이도가 있을것으로 생각된다.</li>
</ul>
</li>
</ul>

      
      
      
      <a href='/tags/%EA%B0%9C%EB%B0%9C'>개발</a>, <a href='/tags/mediapipe'>Mediapipe</a></div>
</main>
</body>

</html>
